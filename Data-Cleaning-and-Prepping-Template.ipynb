{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5072ad7-d902-45ef-9b95-52ddf7c9ee46",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries for data manipulation\n",
    "import pandas as pd\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fef8800e-c9a7-4114-b0cd-f4b18469a611",
   "metadata": {},
   "source": [
    "# Universal Data Cleaning & Preparation Template\n",
    "\n",
    "This Jupyter Notebook provides a step-by-step template for cleaning and preparing any tabular dataset for machine learning. Follow the cells in order to handle missing values, convert data types, and encode categorical features for optimal model performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd9ab9ce-f9d4-4566-94cb-9d2539602b39",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Import necessary libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import confusion_matrix\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20473332-8d92-4832-b5fd-15fad10cd9bc",
   "metadata": {},
   "source": [
    "### Step 1: Load Your Data\n",
    "\n",
    "In the cell below, replace `'your_dataset.csv'` with the path to your own data file. Make sure the data file is in the same folder as this notebook for the easiest setup."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e4f2cc5-57cb-42e8-9715-a930e68d4146",
   "metadata": {},
   "source": [
    "## 1. Data Loading and Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a4961f12-e77c-47f6-99f9-7fd30ceee94d",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# --- Configuration: Please set your file path here ---\n",
    "file_path = 'your_dataset.csv'\n",
    "\n",
    "# --- Data Loading ---\n",
    "try:\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(\"Data loaded successfully!\")\n",
    "    # Display the first 5 rows to confirm\n",
    "    print(df.head())\n",
    "\n",
    "except FileNotFoundError:\n",
    "    print(f\"ERROR: The file '{file_path}' was not found.\")\n",
    "    print(\"Please make sure your data file is in the same directory as this notebook and the file_path variable is set correctly.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2cf27ef2-563c-4656-b52b-9c6379c6351e",
   "metadata": {},
   "source": [
    "### Step 2: Fix Incorrect Data Types\n",
    "\n",
    "This step addresses a common issue where a numeric column is incorrectly read as text (`object`). The code below will convert a specified column to a numeric format. Any values that cannot be converted will be marked as missing data (`NaN`), which we will handle in the next step.\n",
    "\n",
    "**Action Required:** In the code cell below, update the `column_to_fix` variable with the name of the column from your dataset that you need to convert."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b18bf4e-87af-4d53-bf62-8e118782ab05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# The user of this template should change this column name.\n",
    "# We are using 'TotalCharges' because it exists in our current test data.\n",
    "column_to_fix = 'TotalCharges'\n",
    "\n",
    "# --- Execution ---\n",
    "print(f\"Attempting to convert column: '{column_to_fix}'...\")\n",
    "if column_to_fix in df.columns and df[column_to_fix].dtype == 'object':\n",
    "    initial_missing = df[column_to_fix].isnull().sum()\n",
    "    df[column_to_fix] = pd.to_numeric(df[column_to_fix], errors='coerce')\n",
    "    new_missing = df[column_to_fix].isnull().sum()\n",
    "    print(f\"SUCCESS: Column '{column_to_fix}' was converted to a numeric data type.\")\n",
    "    if new_missing > initial_missing:\n",
    "        print(f\"INFO: {new_missing - initial_missing} new missing values (NaN) were created during conversion.\")\n",
    "elif column_to_fix in df.columns:\n",
    "     print(f\"INFO: Column '{column_to_fix}' is already a numeric type. No action needed.\")\n",
    "else:\n",
    "    print(f\"INFO: Column '{column_to_fix}' not found in the DataFrame. Skipping this step.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c2aa71a-2c05-44bb-a887-8e9df0e20ee2",
   "metadata": {},
   "source": [
    "### Step 3: Handle Missing Values\n",
    "\n",
    "Machine learning models cannot work with missing values (NaN). We need to either drop the rows containing them or fill them with a meaningful value (a process called imputation).\n",
    "\n",
    "The code below will automatically find all numeric columns that have missing values and fill them with the **mean** (average) of that column. This is a common and robust strategy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c9ddb3c2-e2a9-4f3d-819d-3e1465a10b2b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This cell finds all numeric columns with missing values and fills them with that column's mean.\n",
    "\n",
    "print(\"--- Missing Values Before Filling ---\")\n",
    "# Let's see which columns have missing values and how many\n",
    "missing_values = df.isnull().sum()\n",
    "print(missing_values[missing_values > 0])\n",
    "\n",
    "# Impute missing values for numeric columns\n",
    "for col in df.select_dtypes(include=np.number).columns:\n",
    "    if df[col].isnull().sum() > 0:\n",
    "        mean_value = df[col].mean()\n",
    "        df[col] = df[col].fillna(mean_value)\n",
    "        print(f\"\\nINFO: Missing values in numeric column '{col}' were filled with its mean: {mean_value:.2f}\")\n",
    "\n",
    "print(\"\\n--- Missing Values After Filling ---\")\n",
    "# Verify that there are no more missing values in numeric columns\n",
    "missing_values_after = df.isnull().sum()\n",
    "print(missing_values_after[missing_values_after > 0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3c96b7b-b554-4fd5-9c66-f8a19cd3e8b7",
   "metadata": {},
   "source": [
    "### Step 4: Encode Categorical Data\n",
    "\n",
    "Finally, we need to convert all text-based columns into numbers for the model. We will use two techniques:\n",
    "1.  **Binary Encoding:** For columns with only two unique values (e.g., 'Yes'/'No'). These will be mapped to `0` and `1`.\n",
    "2.  **One-Hot Encoding:** For columns with more than two categories. This creates new columns for each category with `0` or `1` values.\n",
    "\n",
    "**Action Required:** In the code cell below, update the `target_column` and `binary_cols_to_map` variables to match your dataset's column names."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a0aaddbe-d6e5-4748-b650-e5f9981fdbe7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Configuration ---\n",
    "# The column you want to predict.\n",
    "target_column = 'Churn' \n",
    "\n",
    "# List of columns that have only two text values (e.g., 'Yes'/'No', 'Male'/'Female').\n",
    "binary_cols_to_map = ['gender', 'Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
    "\n",
    "# --- Execution ---\n",
    "# Work on a copy to be safe and keep the original df for reference.\n",
    "df_final = df.copy()\n",
    "\n",
    "print(\"--- Starting Categorical Encoding ---\")\n",
    "\n",
    "# Encode other binary columns\n",
    "for col in binary_cols_to_map:\n",
    "    if col in df_final.columns and df_final[col].dtype == 'object':\n",
    "        unique_vals = df_final[col].unique()\n",
    "        if len(unique_vals) == 2:\n",
    "            mapping = {unique_vals[0]: 0, unique_vals[1]: 1}\n",
    "            df_final[col] = df_final[col].map(mapping)\n",
    "            print(f\"SUCCESS: Binary column '{col}' was encoded using mapping: {mapping}\")\n",
    "\n",
    "# Encode the target column (handle it separately for clarity)\n",
    "if target_column in df_final.columns and df_final[target_column].dtype == 'object':\n",
    "     unique_vals = df_final[target_column].unique()\n",
    "     if len(unique_vals) == 2:\n",
    "        mapping = {unique_vals[0]: 0, unique_vals[1]: 1}\n",
    "        df_final[target_column] = df_final[target_column].map(mapping)\n",
    "        print(f\"SUCCESS: Target column '{target_column}' was encoded using mapping: {mapping}\")\n",
    "\n",
    "# One-Hot Encode all remaining 'object' type columns\n",
    "# These are columns with more than 2 text categories\n",
    "object_cols_to_encode = df_final.select_dtypes(include=['object']).columns\n",
    "if len(object_cols_to_encode) > 0:\n",
    "    df_final = pd.get_dummies(df_final, columns=object_cols_to_encode, drop_first=True)\n",
    "    print(f\"\\nSUCCESS: The following columns were One-Hot Encoded: {list(object_cols_to_encode)}\")\n",
    "else:\n",
    "    print(\"\\nINFO: No multi-category columns found to One-Hot Encode.\")\n",
    "\n",
    "print(\"\\n--- Encoding Complete ---\")\n",
    "print(\"Displaying the first 5 rows of the final, fully numeric DataFrame:\")\n",
    "df_final.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a72bab89-92bc-40a9-bd62-8ef3d927f9fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get a summary of the DataFrame\n",
    "churn_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b387a35-cbb4-44e1-9005-0629da554c15",
   "metadata": {},
   "source": [
    "## 2. Data Preprocessing and Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9326c9d-46c4-4346-9fa0-6e095eb57e60",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fix 'TotalCharges' column and drop rows with missing values\n",
    "churn_df['TotalCharges'] = pd.to_numeric(churn_df['TotalCharges'], errors='coerce')\n",
    "churn_df.dropna(inplace=True)\n",
    "\n",
    "# Encode binary and target variables\n",
    "churn_df['Churn'] = churn_df['Churn'].map({'No': 0, 'Yes': 1})\n",
    "churn_df['gender'] = churn_df['gender'].map({'Female': 0, 'Male': 1})\n",
    "binary_cols = ['Partner', 'Dependents', 'PhoneService', 'PaperlessBilling']\n",
    "for col in binary_cols:\n",
    "    churn_df[col] = churn_df[col].map({'No': 0, 'Yes': 1})\n",
    "\n",
    "# One-Hot Encode multi-category columns and drop customerID\n",
    "multi_cat_cols = churn_df.select_dtypes(include=['object']).columns.drop(['customerID'])\n",
    "churn_df_final = pd.get_dummies(churn_df, columns=multi_cat_cols, drop_first=True)\n",
    "churn_df_final = churn_df_final.drop('customerID', axis=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8d456f0-827d-4450-9491-2e1b606ffdf1",
   "metadata": {},
   "source": [
    "### Final Data Check\n",
    "As a final preprocessing step, we clean the column names for API compatibility and check the data types to ensure everything is numeric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3992fa6e-5037-4805-bfe6-ba5f64fa3d99",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Clean column names\n",
    "churn_df_final.columns = churn_df_final.columns.str.replace(' ', '_')\n",
    "\n",
    "# Verify that all columns are now numeric\n",
    "print(\"Final Data Types:\")\n",
    "churn_df_final.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6adc5328-f66a-4fd9-81e2-8c096e1c007b",
   "metadata": {},
   "source": [
    "## 3. Model Building and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96d7f225-62fd-4a2f-84a3-d14eb8f26f9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Separate features (X) and target (y)\n",
    "X = churn_df_final.drop('Churn', axis=1)\n",
    "y = churn_df_final['Churn']\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42)\n",
    "\n",
    "# Scale the features\n",
    "scaler = StandardScaler()\n",
    "X_train_scaled = scaler.fit_transform(X_train)\n",
    "X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "# Initialize and train the Logistic Regression model\n",
    "log_model = LogisticRegression(max_iter=1000)\n",
    "log_model.fit(X_train_scaled, y_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d11c942-331e-4844-bf32-3609a27f240f",
   "metadata": {},
   "source": [
    "## 4. Model Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "013ba629-07fb-4b5f-b6f0-2a40c93dc226",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluate the model's accuracy\n",
    "accuracy = log_model.score(X_test_scaled, y_test)\n",
    "print(f\"Model Accuracy: {accuracy:.4f}\")\n",
    "\n",
    "# Generate the confusion matrix\n",
    "y_pred = log_model.predict(X_test_scaled)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(cm)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eecdb559-d54f-41de-8636-54bad8f1f986",
   "metadata": {},
   "source": [
    "## 5. Save Model and Scaler\n",
    "The final trained model and the scaler are saved to disk for use in the API."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25b1b8e4-77fb-4d0c-9eca-4ab721949453",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save the model and scaler to disk\n",
    "joblib.dump(log_model, 'churn_model.pkl')\n",
    "joblib.dump(scaler, 'scaler.pkl')\n",
    "\n",
    "print(\"Model and Scaler saved successfully.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444ad8cd-6697-4831-a7ad-b189044857b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
